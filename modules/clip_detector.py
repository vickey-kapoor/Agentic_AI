import torch
import open_clip
from PIL import Image
import numpy as np
import os
import pickle


class CLIPDetector:
    """CLIP-based AI image detector using zero-shot text classification"""

    def __init__(self, model_name='ViT-B-32', pretrained='openai'):
        """
        Initialize CLIP detector.

        Args:
            model_name: CLIP model variant
            pretrained: Pretrained weights to use
        """
        print("Loading CLIP model...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")

        # Load CLIP model
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(
            model_name, pretrained=pretrained
        )
        self.model = self.model.to(self.device)
        self.model.eval()

        # Get tokenizer for text encoding
        self.tokenizer = open_clip.get_tokenizer(model_name)

        # Define text prompts for zero-shot classification
        self.ai_prompts = [
            "an AI-generated image",
            "a synthetic image created by artificial intelligence",
            "a fake image made by AI",
            "an image generated by Midjourney, DALL-E, or Stable Diffusion",
            "a digitally synthesized artificial image",
            "an AI art image with perfect skin and unrealistic details",
            "a deepfake or AI-generated face",
            "computer generated imagery",
        ]

        self.real_prompts = [
            "a real photograph taken by a camera",
            "an authentic photograph of a real scene",
            "a natural photo with real lighting and imperfections",
            "a genuine photograph captured in real life",
            "a real photo with natural camera artifacts",
            "an unedited authentic photograph",
            "a real image with natural noise and imperfections",
            "a candid real-world photograph",
        ]

        # Pre-compute text embeddings
        self.ai_text_features = self._encode_text_prompts(self.ai_prompts)
        self.real_text_features = self._encode_text_prompts(self.real_prompts)

        # Legacy: keep for backwards compatibility with saved databases
        self.real_embeddings = []
        self.ai_embeddings = []
        self.labels = []
        self.nn_model = None

        print("CLIP model loaded successfully!")

    def _encode_text_prompts(self, prompts):
        """Encode a list of text prompts into CLIP text features"""
        with torch.no_grad():
            tokens = self.tokenizer(prompts).to(self.device)
            text_features = self.model.encode_text(tokens)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            return text_features

    def extract_features(self, image):
        """
        Extract CLIP features from an image.

        Args:
            image: PIL Image object

        Returns:
            torch tensor of features (normalized)
        """
        with torch.no_grad():
            # Preprocess image
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)

            # Extract features
            features = self.model.encode_image(image_tensor)

            # Normalize
            features = features / features.norm(dim=-1, keepdim=True)

            return features

    def analyze_image(self, image):
        """
        Analyze an image to determine if it's AI-generated using zero-shot classification.

        Args:
            image: PIL Image object

        Returns:
            dict: {
                'is_ai': bool,
                'confidence': float (0-1),
                'verdict': str,
                'full_analysis': str
            }
        """
        # Extract image features
        image_features = self.extract_features(image)

        # Calculate similarity to AI prompts
        ai_similarities = (image_features @ self.ai_text_features.T).squeeze(0)
        ai_score = ai_similarities.mean().item()
        ai_max = ai_similarities.max().item()

        # Calculate similarity to real prompts
        real_similarities = (image_features @ self.real_text_features.T).squeeze(0)
        real_score = real_similarities.mean().item()
        real_max = real_similarities.max().item()

        # Calculate combined score using softmax-style normalization
        # Higher temperature = more balanced, lower = more decisive
        temperature = 0.5
        ai_exp = np.exp(ai_score / temperature)
        real_exp = np.exp(real_score / temperature)

        ai_probability = ai_exp / (ai_exp + real_exp)

        # Also consider max scores for edge cases
        max_diff = ai_max - real_max

        # Combine average and max for final decision
        combined_score = 0.7 * ai_probability + 0.3 * (0.5 + max_diff)
        combined_score = max(0, min(1, combined_score))  # Clamp to [0, 1]

        # Determine verdict with adjusted thresholds
        if combined_score >= 0.55:
            verdict = 'Likely AI'
            is_ai = True
            confidence = combined_score
        elif combined_score <= 0.45:
            verdict = 'Likely Real'
            is_ai = False
            confidence = 1 - combined_score
        else:
            verdict = 'Uncertain'
            is_ai = False
            confidence = 0.5

        analysis = f"AI score: {ai_score:.3f} (max: {ai_max:.3f}), Real score: {real_score:.3f} (max: {real_max:.3f}), Combined: {combined_score:.3f}"

        return {
            'is_ai': is_ai,
            'confidence': confidence,
            'verdict': verdict,
            'full_analysis': analysis
        }

    def add_reference_image(self, image, is_ai):
        """Legacy method for backwards compatibility"""
        features = self.extract_features(image).cpu().numpy().flatten()
        if is_ai:
            self.ai_embeddings.append(features)
        else:
            self.real_embeddings.append(features)
        self.labels.append(1 if is_ai else 0)

    def add_reference_folder(self, folder_path, is_ai):
        """Legacy method for backwards compatibility"""
        valid_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
        for filename in os.listdir(folder_path):
            ext = os.path.splitext(filename)[1].lower()
            if ext in valid_extensions:
                try:
                    img_path = os.path.join(folder_path, filename)
                    image = Image.open(img_path).convert('RGB')
                    self.add_reference_image(image, is_ai)
                    print(f"Added: {filename} ({'AI' if is_ai else 'Real'})")
                except Exception as e:
                    print(f"Error loading {filename}: {e}")

    def _rebuild_nn_model(self):
        """Legacy method - no longer used for classification"""
        pass

    def save_database(self, filepath):
        """Save the reference database to disk (legacy support)"""
        data = {
            'real_embeddings': self.real_embeddings,
            'ai_embeddings': self.ai_embeddings,
            'labels': self.labels
        }
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
        print(f"Database saved to {filepath}")

    def load_database(self, filepath):
        """Load a reference database from disk (legacy support)"""
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                data = pickle.load(f)
            self.real_embeddings = data.get('real_embeddings', [])
            self.ai_embeddings = data.get('ai_embeddings', [])
            self.labels = data.get('labels', [])
            print(f"Loaded {len(self.real_embeddings)} real and {len(self.ai_embeddings)} AI reference images")
            return True
        return False

    def get_stats(self):
        """Get database statistics"""
        return {
            'real_count': len(self.real_embeddings),
            'ai_count': len(self.ai_embeddings),
            'total': len(self.labels)
        }
